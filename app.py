import streamlit as st
import cv2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models import vgg16
from PIL import Image
from facenet_pytorch import MTCNN

# ì–¼êµ´ ì¸ì‹ ëª¨ë¸ í´ëž˜ìŠ¤
class FaceRecognitionModel(nn.Module):
    def __init__(self):
        super(FaceRecognitionModel, self).__init__()
        self.model = vgg16(pretrained=False)
        self.model.classifier = nn.Sequential(
            *list(self.model.classifier.children())[:-1]
        )

    def forward(self, x):
        return self.model(x)

    def load_state_dict(self, state_dict):
        # ë¡œë“œí•  ë•Œ ì‚¬ìš©ë˜ëŠ” í‚¤ ì´ë¦„ì„ ìˆ˜ì •í•˜ì—¬ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("features.", "model.features.")  
            new_key = new_key.replace("classifier.", "model.classifier.")
            if "model.classifier.6" in new_key:
                continue
            new_state_dict[new_key] = value
            
        super(FaceRecognitionModel, self).load_state_dict(new_state_dict)


# ì–¼êµ´ ì¸ì‹ í•¨ìˆ˜
known_face_encodings = []  # ì´ˆê¸°í™”

known_face_encodings = []  # ì´ˆê¸°í™”
known_face_names = []      # ì´ˆê¸°í™”

def recognize_face(model, face, known_face_encodings, known_face_names):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    face = Image.fromarray(face)
    face = transform(face).unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        face_encoding = model(face).cpu().numpy().flatten()
    
    # known_face_encodingsì™€ known_face_namesì— ìƒˆë¡œìš´ ì–¼êµ´ ì¸ì½”ë”©ê³¼ ì´ë¦„ ì¶”ê°€
    known_face_encodings.append(face_encoding)
    known_face_names.append("New Face")

    matches = np.linalg.norm(known_face_encodings - face_encoding, axis=1)
    name = "Unknown"
    
    if len(matches) > 0:
        min_distance_index = np.argmin(matches)
        if matches[min_distance_index] < 0.6:  # ìž„ê³„ê°’
            name = known_face_names[min_distance_index]
    
    return name

# Streamlit ì•± ë©”ì¸ í•¨ìˆ˜
def main():
    st.title("ðŸ“¸ AI ì–¼êµ´ì¸ì‹ ì¶œê²°ê´€ë¦¬ ì‹œìŠ¤í…œ ðŸ“¸")
    run = st.checkbox('ì›¹ìº  ì‹œìž‘/ì •ì§€')

    # ì‚¬ì´ë“œë°”
    st.sidebar.subheader("CVíŠ¸ëž™ ì¶œì„ë¶€")

    FRAME_WINDOW = st.image([])

    # MTCNN ë¡œë“œ
    mtcnn = MTCNN(keep_all=True, device=DEVICE)

    camera = cv2.VideoCapture(0)

    # ëª¨ë¸ ë¡œë“œ
    model_save_path = './model_pth/best_model.pth'
    model = FaceRecognitionModel().to(DEVICE)
    model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))

    model.eval()

    # ì•Œë ¤ì§„ ì–¼êµ´ ì¸ì½”ë”© ë° ì´ë¦„ (ì´ ì˜ˆì œì—ì„œëŠ” ë¹„ì–´ ìžˆìŒ)
    known_face_encodings = []  # numpy ë°°ì—´ë¡œ ì €ìž¥ëœ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸
    known_face_names = []      # ì¸ì½”ë”©ì— ëŒ€ì‘ë˜ëŠ” ì´ë¦„ ë¦¬ìŠ¤íŠ¸

    while run:
        ret, frame = camera.read()
        if not ret:
            st.write("ì¹´ë©”ë¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # ì¢Œìš° ë°˜ì „
        frame = cv2.flip(frame, 1)

        # ì–¼êµ´ ê²€ì¶œ
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        boxes, _ = mtcnn.detect(rgb_frame)
        
        # ì–¼êµ´ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸° ë° í…ìŠ¤íŠ¸ ì¶”ê°€
        if boxes is not None:
            for box in boxes:
                x, y, w, h = map(int, box)
                face = frame[y:h, x:w]
                name = recognize_face(model, face, known_face_encodings, known_face_names)
                cv2.rectangle(frame, (x, y), (w, h), (255, 0, 0), 2)
                cv2.putText(frame, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)
        
        # BGRì—ì„œ RGBë¡œ ë³€í™˜
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        FRAME_WINDOW.image(frame)

    camera.release()

if __name__ == '__main__':
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    main()
